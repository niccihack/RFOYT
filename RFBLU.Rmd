---
title: "RFBLU-CCFRP"
author: "Nicole Hack"
date: '`r format(Sys.Date(), "%B %d, %Y")`'
output:
  html_document: default
  html_notebook: default
  pdf_document: default
  word_document: default
---

```{r setup, include=FALSE}
#Started 1 Feb 2017
knitr::opts_chunk$set(echo = F, message = F, warning = F)
```
```{r, echo=F, include=F}
packages <- c('readxl','tidyverse','readr','data.table','dplyr','car','ggplot2','ggmap','nortest','broom','lme4')
lapply(packages, require, character.only = T)
```
```{r Data}
#Load my data
Fishing_data <- read_csv("C:/Users/manic/Dropbox/iRock/CCFRP blood samples/Fishing data.txt")
#load CCFRP data
Hack_Sample_Data <- read_csv("C:/Users/manic/Dropbox/Home computer/CalPoly/Rockfish/Field Results/CCFRP_Final_Hacked.csv")


#####Formatting my data#####
#adjust my data to match CCFRP
#because cell is coded as site.cell without a deliminiter, I have to recreate this in my data
ahackdat<-Fishing_data#adjusted Fishing data
ahackdat["SiteCell"]<-ifelse(ahackdat$Location=='PBL',paste('BL'),paste('PB'))
#adding zeros to single cell numbers
ahackdat['ncell']<-ifelse(nchar(ahackdat$Cell)==1,
                       c(paste0('0',ahackdat$Cell)),
                       ahackdat$Cell
                       )

ahackdat['sicell']<-paste(ahackdat$SiteCell,ahackdat$ncell, sep='')
#date column registered as date by R
ahackdat$Date<-as.Date(ahackdat$Date,format="%m/%d/%Y")
#reorder to merge in correct order to match CCFRP data
ahackdat<-ahackdat[,c('Date','Location','Protection','sicell','Drift','Species','Length','TagNum','Bloodsamp')]



#####Formatting CCFRP data#####
#change species codes to match my data
##will need to add more once I get all the data!
fccfrp<-Hack_Sample_Data#fixed CCFRP data
fccfrp$Species<-car::recode(fccfrp$SpeciesCode,"
  'BLA'='RFBLK';
  'BLU'='RFBLU';
  'CPR'='RFCOP';
  'GPR'='RFGOP';
  'KLP'='RFKLP';
  'LCD'='LNGCD';
  'OLV'='RFOLV';
  'VER'='RFVER';
  'CBZ'='SCCAB';
  'DEA'='RFDEC';
  'YTL'='RFYTL'
")
#change area code to match my location code
fccfrp$Area<-car::recode(fccfrp$Area,"
    'BL'='PBL';
    'PB'='PBN'
")
#fix to date function
#combine to get MM/DD/YYYY format
fccfrp$Date<-paste(paste(fccfrp$Month,fccfrp$Day,fccfrp$Year, sep='/'),sep='')
fccfrp$Date<-as.Date(fccfrp$Date,format="%B/%d/%Y")
#change tagID to not be a character with a fucking letter 'O' at the end
fccfrp$TagID<-as.numeric(substr(fccfrp$TagID,1,nchar(fccfrp$TagID)-1))
#reorder columns to combime in proper order
fccfrp <- select(fccfrp,'Date','Area','Site','CellID','Drift','Species','Length','TagID',c(11:29))


#only have igf concentration for blue rockfish so subseting those
igf_data <- read_excel("C:/Users/manic/Dropbox/Home computer/CalPoly/Rockfish/Field Results/RockFish_Hack_2016_stanIGF_022417brb (1).xlsx")

igf_data$sample = igf_data$`Phys Number`
igf_data$`Phys Number`=NULL
igf1 = igf_data %>% 
  filter(igf_data[,1]!=116) %>% #remove duplicated bloodsample #277
  dplyr::select(sample,stanIGF)#only want samplenum and igf conc.
ahackdat$Bloodsamp = as.numeric(ahackdat$Bloodsamp)
samphackdat= right_join(ahackdat, igf1, by = c('Bloodsamp' = 'sample'))
```

#Data summary

278 Total bloodsamples with Igf concentrations

```{r}
#subset those with tags
tagcc<-subset(fccfrp,!is.na(TagID))
#subset those with tag#s
tagnh<-filter(samphackdat,!is.na(TagNum))
#combine
mtfish <- inner_join(tagcc, tagnh, by = c('TagID' = 'TagNum'))
#only one tagged blue, not found in ccfrp data
```
```{r}
#merge columns in CCFRP data in order to combine with my data
fccfrp = unite(fccfrp,id,  1:7, sep = ' ')
fccfrp = fccfrp[!duplicated(fccfrp),]#remove duplicates
#Although there may be slightly different data between duplicated, the id ensures that the variance is restrained to a maximum of 45 min (total time fishing in cell) and a distance of 500 m (total length of cell)

#merge columns in my data to combine with ccfrp data
samphackdat = unite(samphackdat,id,  1:7, sep = ' ', remove = F)
#unite!
mdat = inner_join(samphackdat, fccfrp, by = 'id')

#Many not matched due to differing length inputs
#First round of unmatched data
hackdat1 = anti_join(samphackdat,mdat, by = 'id')
#If there is no start time then increase the length by one, replace id, and rejoin data
misdat = hackdat1 %>% 
  mutate(len = Length+1) %>%
  unite(idp1,c(2:7,12), sep = ' ', remove = F) %>% 
  inner_join(fccfrp, by= c('idp1'='id'))

#Still some missing
hackdat2 = anti_join(hackdat1,misdat,by= 'id')
mdat2 = hackdat2 %>%
  mutate(len = Length-1) %>%
  unite(idp2,c(2:7,12), sep = ' ', remove = F) %>% 
  inner_join(fccfrp, by= c('idp2'='id'))

#Ones still missing
lost = anti_join(hackdat2,mdat2,by = 'id')
#See if any match without a drift number
dccfrp = fccfrp %>% separate(id,c('Date','Location','Protection','sicell','Drift','species','length'),sep = ' ') %>% unite(idd,c(1:4,6,7), sep = ' ')#make new id for ccfrp without drift num
#make new id for my data
lost = lost %>% unite(idd,c(2:5,7,8), sep = ' ', remove = F) 
#match
lostdat = dplyr::inner_join(lost,dccfrp, by = 'idd') 
#created duplicates due to multiple entries in dccfrp
lostdat = lostdat[!duplicated(lostdat$Bloodsamp),]
#Still some missing
#No drift, length-1
hackdat3 = anti_join(lost,lostdat,by = 'Bloodsamp')
lostdat2 = hackdat3 %>%
  mutate(len = Length-1) %>%
  unite(idp2,c(3:6,8,13), sep = ' ', remove = F) %>% 
  inner_join(dccfrp, by= c('idp2'='idd'))
lostdat2 = lostdat2[!duplicated(lostdat2$Bloodsamp),]

#No drift, length+1
hackdat4 = anti_join(hackdat3,lostdat2, by = 'Bloodsamp')
lostdat3 = hackdat4 %>%
  mutate(len = Length+1) %>%
  unite(idp2,c(3:6,8,13), sep = ' ', remove = F) %>% 
  inner_join(dccfrp, by= c('idp2'='idd'))

#Use ID information for the rest of the unmatched data

#merge all matched data
#need all columns to match
misdat = misdat[,c(2:8,10:33)] %>% dplyr::rename(Length = len, id = idp1)
mdat2 = mdat2[,c(2:8,10:33)] %>% dplyr::rename(Length = len, id = idp2)
lostdat = lostdat[,c(3:6,13,8:12,14:33)] %>%
  unite(id,c(1:7), sep = ' ', remove = F) %>% 
  dplyr::rename(Drift = Drift.y)
lostdat$Drift = as.numeric(lostdat$Drift)
lostdat2 = lostdat2[,c(4:7,15,9,14,11:13,16:35)]%>%
  unite(id,c(1:7), sep = ' ', remove = F) %>% 
  dplyr::rename(Drift = Drift.y, Length = len)
lostdat2$Drift = as.numeric(lostdat2$Drift)
lostdat3 = lostdat3[,c(4:7,15,9,14,11:13,16:35)]%>%
  unite(id,c(1:7), sep = ' ', remove = F) %>% 
  dplyr::rename(Drift = Drift.y, Length = len)
lostdat3$Drift = as.numeric(lostdat3$Drift)
matched = dplyr::bind_rows(mdat,misdat,mdat2,lostdat,lostdat2,lostdat3)
```

266 samples matched to CCFRP data

##Check for outliers and normality
```{r}

#find outliers
igfremoved = matched %>% group_by(Location,Protection) %>%
  filter((stanIGF < (mean(stanIGF)-(3*sd(stanIGF)))) |  (stanIGF > (3*sd(stanIGF)+mean(stanIGF))))

knitr::kable(igfremoved[,c(1,10:11)], caption = 'Outliers removed')

#Removing #124 due to outlier
matched <- filter(matched, Bloodsamp != 124)
```

```{r, include=F}
#Run linear regression to correct for length
igf.lm <- lm(stanIGF~factor(Length), data = matched)
shapiro.test(igf.lm$residuals)
ggplot(igf.lm, aes(x = .resid))+
  geom_histogram()
```

```{r, include=F}
#removing nas
matched <- filter(matched, !is.na(stanIGF))
#Square root transforming data to normalize
matched <- mutate(matched, igf = sqrt(stanIGF))
new.lm = lm(igf~factor(Length), data = matched)
shapiro.test(new.lm$residuals)


write.csv(matched,'RFBLU-CCFRP.csv')
```

#Models

```{r}
library(GGally)
subdat <- matched[,c(2:4,8,11)]
#ggpairs(subdat,aes(color = Protection))
```

```{r, include = F}
#ANOVAs
matched$cDate = as.character(matched$Date)

#testing only location and protection
dat_lm = lm(igf~Location+Protection, data = matched)
summary(dat_lm)
anova(dat_lm)
library(agricolae)
tukey = HSD.test(dat_lm,c('Protection','Location'))
knitr::kable(tukey$groups,caption = 'Post-hoc Tukey HSD test')

dat_lmer_int = lm(igf~Location*Protection+(Length), data = matched)
summary(dat_lmer_int)
```

```{r, include = F}
#Location and protection nested in date
dat_sim = lm(igf~Date/(Location*Protection)+Length, data = matched)
summary(dat_sim)
anova(dat_sim)
```

Correct for length
```{r}
dat_len = lm(stanIGF~Length, data = matched)
sum_datlen = summary(dat_len)
#Add residuals to orginal dataframe
matched <- matched %>% modelr::add_residuals(dat_len)
```

```{r}
ggplot(matched, aes(y = stanIGF, x = Length)) + 
  geom_point(aes(fill = Protection), shape = 21, color =('black'), size = 3) +
  scale_fill_manual(name = 'Protection', values = c("#D10000","#004C99"), labels = c('MPA','REF'))+
  stat_smooth(method = 'lm',data = matched,color = 'black',size = 2, fullrange = T)+
  scale_x_continuous(expand=c(0,0)) +
  scale_y_continuous(expand=c(0,0)) +
  coord_cartesian(xlim=c(10,40), ylim=c(0,60)) +
  ylab('Plasma Igf1 (ng / mL)')+
  xlab('Length (cm)')+
  annotate('text',x = 15, y = 55, label = paste("r^2 == ", format(sum_datlen$adj.r.squared, digits = 2)), parse = T)+
  theme_classic(base_size = 20)+
  theme(legend.background = element_rect(linetype = 'solid', size = 0.5, colour = 'black'),legend.position = c(0.8,0.9),plot.margin=unit(c(1,1,1.5,1.2),"cm"))
ggsave(filename = 'igf-len_lm.jpeg', height = 5, width = 7)
```


Residuals vs Protection & Location

```{r}
#Run model with residuals
lm.res = lm(resid~Protection*Location, data = matched)
anova(lm.res)
```

Check for normality

```{r, Normality}
shapiro.test(lm.res$residuals)
ggplot(lm.res, aes(x = .resid))+geom_histogram()

#Need to get rid of negatives for stats and graphs
matched <- mutate(matched, stdIgf = (max(matched$resid)+resid))
```

As this data looks normal and only does not pass the normality test due to a large sample size, I did not transform the data.


##Date effect

*Using residuals

```{r}
#Effect of date
ggplot(matched, aes(x = Date, y = stdIgf, color = Protection))+
  geom_point()+
  ylab('Residual IGF1 concentrations')+
  theme_classic()

aov.date = lm(resid~cDate, data = matched)
sum_date = summary(aov.date)
anova(aov.date)
```

Residuals vs Protection and Location with Date 

```{r}
res.pro.loc.date = lm(resid~Protection*Location*Date, data = matched)
anova(res.pro.loc.date)
```

Date does not interact with Location or Preotection

```{r}
res.pro.loc.date = lm(resid~Protection*Location+Date, data = matched)
anova(res.pro.loc.date)
anova(lm.res,res.pro.loc.date)
```

Date signifcantly affects model

**BUT** really our model should reflect our sampling methods with Protection and Location nested within Date as we only sample one Location x Protection each day.

```{r}
res.pro.loc.date = lm(dat_len$residuals~Date/(Location*Protection), data = matched)
anova(res.pro.loc.date)
```

Now we can add Time

```{r}
#some time stamps not in military time
matched = dplyr::rename(matched, St_time = `Start Time`, En_time = `End Time`)
test = matched %>% dplyr::filter(lubridate::hour(matched$St_time) < 7)  %>% dplyr::select(id,St_time,En_time)
write.csv(test, 'RFBLUtime-in-pm.csv')
#Try to fix times...following code does not work due to pipe. strptime does not want to accept piped vectors
#ifelse((lubridate::hour(test$St_time) < 7),(test$St_time = paste(test$St_time,'PM', sep = " ") %>% format(strptime(test$St_time, "%I:%M:%S %p"), format="%H:%M:%S")),(test$St_time))

#Replace wrong data with NAs for now
matched.na <- matched[!(matched$id %in% test$id),]
#rerun model with new data
dat_len_na = lm(stanIGF~Length, data = matched.na)
res.time = lm(dat_len_na$residuals~Date/(Location*Protection)+St_time, data = matched.na)
summary(res.time)
```

Time is not significant in the model

#Graphing

*With residuals*
```{r, Residuals by location}

#Bargraph of IGF1 by location and protection
sumresfish <- matched %>% 
  group_by(Location,Protection) %>%
  dplyr::summarize(mean = mean(stdIgf), sem =  (sd(stdIgf)/sqrt(length(stdIgf))), mean.len = mean(Length), sem.len = (sd(Length)/sqrt(length(Length))), n=length(stdIgf))

summary(lm(stdIgf~Location/Protection-1, data = matched))

library(ggsignif)
ggplot(sumresfish, aes(x = Location, weight = mean, ymin=mean - sem, ymax=mean + sem, fill = Protection))+
  geom_bar(stat = 'identity', color = 'black',width = 0.6, aes(y=mean),position=position_dodge(width = 0.6))+
  geom_errorbar(width = 0.25, position=position_dodge(width=0.6))+
  ylab(expression('Plasma Igf1'['STD']))+
  coord_cartesian(ylim = c(25,40))+
  scale_fill_manual(values = c("#D10000","#004C99"))+
  geom_signif(aes(y = mean), y_position=c(36,29), xmin=c(0.8, 1.8), xmax=c(1.2, 2.2),annotation=c("***", "NS"), tip_length=0) +
  geom_signif(comparisons = list(c("PBL", "PBN")), test = t.test,aes(y = mean),
              map_signif_level=TRUE, annotation = '***', y_position = 38)+
  theme_classic(base_size = 18)+
  theme(plot.margin=unit(c(1,1,1.5,1.2),"cm"))
ggsave(filename = 'Res.Loc.tiff', width = 7, height = 5)
```



*With raw data*

```{r, include=F}
#Bargraph of IGF1 by location and protection
sumfish <- matched %>% dplyr::filter(!is.na(stanIGF)&!is.na(Length)) %>% group_by(Location,Protection) %>% dplyr::summarize(mean = mean(stanIGF), sem =  (sd(stanIGF)/sqrt(length(stanIGF))), mean.len = mean(Length), sem.len = (sd(Length)/sqrt(length(Length))), n=length(stanIGF))

summary(lm(igf~Location/Protection-1, data = matched))

library(ggsignif)
ggplot(sumfish, aes(x = Location, weight = mean, ymin=mean - sem, ymax=mean + sem, fill = Protection))+
  geom_bar(stat = 'identity', color = 'black',width = 0.6, aes(y=mean),position=position_dodge(width = 0.6))+
  geom_errorbar(width = 0.25, position=position_dodge(width=0.6))+
  ylab('Plasma Igf1 (ng / ml)')+
  coord_cartesian(ylim = c(15,35))+
  scale_fill_manual(values = c("#D10000","#004C99"))+
  geom_signif(aes(y = mean), y_position=c(28,20), xmin=c(0.8, 1.8), xmax=c(1.2, 2.2),annotation=c("*", "NS"), tip_length=0) +
  geom_signif(comparisons = list(c("PBL", "PBN")), test = t.test,aes(y = mean),
              map_signif_level=TRUE, annotation = '***', y_position = 33)+
  theme_classic(base_size = 18)+
  theme(plot.margin=unit(c(1,1,1.5,1.2),"cm"))
ggsave(filename = 'Plot1.tiff', width = 7, height = 5)
```

Length by Location/Protection

```{r, Length}
summary(lm(Length~Location/Protection, data = matched))

ggplot(sumfish, aes(x = Location, weight = mean.len, ymin=mean.len - sem.len, ymax=mean.len + sem.len, fill = Protection))+
  geom_bar(stat = 'identity', color = 'black',width = 0.6, aes(y=mean.len),position=position_dodge(width = 0.6))+
  geom_errorbar(width = 0.25, position=position_dodge(width=0.6))+
  ylab('Length (cm)')+
  coord_cartesian(ylim = c(15,35))+
  scale_fill_manual(values = c("#D10000","#004C99"))+
  geom_signif(aes(y = mean.len), y_position=c(26,22), xmin=c(0.8, 1.8), xmax=c(1.2, 2.2),annotation=c("NS", "NS"), textsize = 6, tip_length=0) +
  geom_signif(comparisons = list(c("PBL", "PBN")), test = t.test,aes(y = mean.len),
              map_signif_level=TRUE, annotation = '**',textsize = 6, y_position = 30)+
  stat_count(aes(label = n), geom = "text",size = 6, color='white', position=position_dodge(width = 0.6), vjust=2)+
  theme_classic(base_size = 18)+
  theme(plot.margin=unit(c(1,1,1.5,1.2),"cm"))
ggsave(filename = 'Plot2.tiff', width = 7, height = 5)

all = matched %>% group_by(Location,Protection) 
all.lm = lm(igf~Location+Protection + factor(Length), data = all)
library(emmeans)
all.lsm = emmeans(all.lm, c('Location','Protection'))
library(multcompView)
cld(all.lsm,Letters=letters)#adds letters and makes it readable
```

##Piedras Blancas

*With residuals

```{r PBL}

PBL <- matched %>% filter(!is.na(stdIgf) & Location == 'PBL') %>% group_by(Location,Protection,Date) %>% dplyr::summarize(mean.igf = mean(stdIgf), sem.igf =  (sd(stdIgf)/sqrt(length(stdIgf))),mean.len = mean(Length), sem.len = (sd(Length)/sqrt(length(Length))), n=length(stanIGF)) 
PBL$text_date = as.character(PBL$Date)
PBL$group = c('1','2','1','2')

PBLdat <- matched %>% filter(!is.na(resid) & Location == 'PBL')
summary(lmer(resid~Protection+(1|Date),data = PBLdat))
```

Protection is not significant if you take into account Date as a random effect.

###Igf

*Using residuals

Separate date anovas

First 2 days (08/01-02)
```{r}
PBLdat$Date_c = as.character(PBLdat$Date)#adding character of date for lsmeans
PBL.one <- filter(PBLdat,Date_c == '2016-08-01' | Date_c == '2016-08-02')
anova(lm(stdIgf~Date, data = PBL.one))
```

Last 2 days (08/15&17)
```{r, warning = F}
PBL.two <- filter(PBLdat,Date_c == '2016-08-15' | Date_c == '2016-08-17')
anova(lm(stdIgf~Date, data = PBL.two))
ggplot(PBL, aes(x = text_date, weight = mean.igf, ymin=mean.igf - sem.igf, ymax=mean.igf + sem.igf, fill = Protection))+
  geom_bar(stat = 'identity', color = 'black', aes(y=mean.igf))+
  geom_errorbar(width = 0.25)+
  ylab(expression('Plasma Igf1'['STD']))+
  xlab('')+
  coord_cartesian(ylim = c(20,45))+
  facet_grid(.~group, scales = 'free_x')+
  scale_fill_manual(values = c("#D10000","#004C99"))+
  geom_signif(data=PBL,aes(y_position=c(40,40,37,37),xmin=c(0.8,0.8,0.8,0.8), xmax=c(2.2,2.2,2.2,2.2), annotations=c('*','*','NS','NS')),tip_length = 0,textsize = 6, manual=T) +
  stat_count(aes(label = n), geom = "text",size = 6, color='white', position=position_dodge(width = 0.6), vjust=2.4)+
  theme_classic(base_size = 18)+
  theme(plot.title = element_text(hjust = 0.5),axis.text.x = element_text(angle = 45, hjust = 1), plot.margin = unit(c(1,1,1.5,1.2),"cm"), strip.text.x = element_blank())
ggsave(filename = 'PBL-igf1.tiff', width = 7, height = 5)
```

*Using square root transformed raw data

First 2 days (08/01-02)
```{r}
pbl1_len = lm(igf~Length, data = PBL.one)
summary(pbl1_len)
ggplot(PBL.one, aes(x=Length, y=igf))+geom_point()+geom_smooth(method='lm')+theme_classic()
```

Last 2 days (08/15&17)
```{r}
pbl2_len = lm(igf~Length, data = PBL.two)
summary(pbl2_len)
ggplot(PBL.two, aes(x=Length, y=igf))+geom_point()+geom_smooth(method='lm')+theme_classic()
```

Comparing between two regressions
```{r}
#need to make column with labeled groups
PBLdat = PBLdat %>% group_by(Date) %>% dplyr::mutate(group = ifelse(lubridate::day(Date)<5,paste('1'),paste('2')))
anova(lm(igf~Length+group, data = PBLdat))
ggplot(PBLdat, aes(x = Length, y=igf, color = group))+
  geom_point()+
  geom_smooth(method = 'lm')+
  scale_color_manual(values = c("orange","brown"), name = '', labels = c('Aug 8-9', 'Aug 15-17'))+
  theme_classic()
```

Groups of dates have significantly different slopes but the issue is they also have different lengths.

```{r}
summary(lm(Length~group, data = PBLdat))
ggplot(PBLdat, aes(x = group, y=Length, color = group))+
  geom_boxplot()+
  geom_smooth(method = 'lm')+
  scale_color_manual(values = c("orange","brown"))+
  scale_x_discrete(labels = c('Aug 8-9','Aug 15-17'))+
  xlab('')+
  theme_classic()+
  theme(legend.position = 'none')
```

Running regression on *non-normalized* igf1 and saving residuals

Aug 08-09
```{r}
pbl1_res <- lm(stanIGF~Length, data = PBL.one)
summary(pbl1_res)
PBL.one <- PBL.one %>% modelr::add_residuals(pbl1_res)
pls::predplot(pbl1_res)
```

Aug 15-17
```{r}
pbl2_res <- lm(stanIGF~Length, data = PBL.two)
summary(pbl2_res)
PBL.two <- PBL.two %>% modelr::add_residuals(pbl2_res)
pls::predplot(pbl2_res)
```

I don't know if you want to do that if length and igf don't correlate.

###Length 

```{r}
anova(lm(Length~Date*Protection, data = PBLdat))
PBL.date = (lm(Length~Date, data = PBLdat))
HSD.test(PBL.date,'Date')
anova(lmer(Length~Protection+(1|Date), data = PBLdat))
```

Separate date anovas
First 2 days (08/01-02)
```{r}
anova(lm(Length~Date, data = PBL.one))
```

Last 2 days (08/15&17)
```{r}
anova(lm(Length~Date, data = PBL.two))
```

While date has a significant effect on length, protection is not significant when date is in the model as a random effect.

```{r Length plot}
ggplot(PBL, aes(x = text_date, weight = mean.len, ymin=mean.len - sem.len, ymax=mean.len + sem.len, fill = Protection))+
  geom_bar(stat = 'identity', color = 'black', aes(y=mean.len))+
  geom_errorbar(width = 0.25)+
  ylab('Length (cm)')+
  xlab('')+
  coord_cartesian(ylim=c(15,30))+
  facet_grid(.~group, scales = 'free_x')+
  scale_fill_manual(values = c("#D10000","#004C99"))+
  geom_signif(data=PBL,aes(y_position=c(26,26,23,23),xmin=c(0.8,0.8,0.8,0.8), xmax=c(2.2,2.2,2.2,2.2), annotations=c('*','*','NS','NS')),tip_length = 0,textsize = 6, manual=T) +
  stat_count(aes(label = n), geom = "text",size = 6, color='white', position=position_dodge(width = 0.6), vjust=2)+
  theme_classic(base_size = 18)+
  theme(plot.title = element_text(hjust = 0.5),axis.text.x = element_text(angle = 45, hjust = 1), plot.margin = unit(c(1,1,1.5,1.2),"cm"), strip.text.x = element_blank())
ggsave(filename = 'PBL-length.tiff', width = 7, height = 5)
```

###LSmeans

This is using raw data and not the residuals.

```{r}
PBL.lm = lm(igf~Date_c + factor(Length), data = PBLdat)
library(emmeans)
PBL.lsm = emmeans(PBL.lm, 'Date_c') 
library(multcompView)
cld(PBL.lsm,Letters=letters)#adds letters and makes it readable
```

Separate ANOVAs

First 2 days
```{r}
anova(lm(igf~Date_c + factor(Length), data = PBL.one))
```

Last 2 days
```{r}
anova(lm(igf~Date_c + factor(Length), data = PBL.two))

#For graphing
PBL.lm.graph = lm(stanIGF~Date_c+factor(Length), data = PBLdat)
PBL.graph = cld(emmeans(PBL.lm.graph,'Date_c'))
PBL.graph$Protection = c('REF','REF','MPA','MPA')
PBL.graph$group=c(2,1,2,1)

ggplot(PBL.graph, aes(x = Date_c, weight = emmean, ymin=emmean - SE, ymax=emmean + SE, fill = Protection))+
  geom_bar(stat = 'identity', color = 'black', aes(y=emmean))+
  geom_errorbar(width = 0.25)+
  ylab('LSmeans Igf1 (ng / ml)')+
  xlab('')+
  coord_cartesian(ylim=c(15,35))+
  facet_grid(.~group, scales = 'free_x')+
  scale_fill_manual(values = c("#D10000","#004C99"))+
  geom_signif(data=PBL.graph,aes(y_position=c(32,32,28,28),xmin=c(0.8,0.8,0.8,0.8), xmax=c(2.2,2.2,2.2,2.2), annotations=c('NS','NS','NS','NS')),tip_length = 0,textsize = 6, manual=T) +
  theme_classic(base_size = 18)+
  theme(plot.title = element_text(hjust = 0.5),axis.text.x = element_text(angle = 45, hjust = 1), plot.margin = unit(c(2,1,1,1),"cm"), strip.text.x = element_blank())
ggsave(filename = 'PBL-lsmeans.tiff', width = 7, height = 5)

```

##Point Buchon

###Igf

*With residuals

```{r PBN}

PBN <- matched %>% filter(!is.na(stdIgf) & !is.na(Length) & Location == 'PBN') %>% group_by(Location,Protection,Date) %>% dplyr::summarize(mean.igf = mean(stdIgf), sem.igf =  (sd(stdIgf)/sqrt(length(stdIgf))),mean.len = mean(Length), sem.len = (sd(Length)/sqrt(length(Length))), n=length(stdIgf)) 
PBN$text_date = as.character(PBN$Date)
PBN$group = c('1','2','1','2')


PBNdat <- matched %>% filter(!is.na(stdIgf) & !is.na(Length) & Location == 'PBN') %>% group_by(Date) %>% mutate(group = as.character(month(Date)))
anova(lm(stdIgf~Date*Protection, data = PBNdat))
```

Separate dates

First 2 days (08/08-09)
```{r}
PBNdat$Date_c = as.character(PBNdat$Date)#adding character of date for lsmeans
PBN.one <- filter(PBNdat,Date_c == '2016-08-09' | Date_c == '2016-08-08')
anova(lm(stdIgf~Date, data = PBN.one))
```

Last 2 days (09/08-09)
```{r}
PBN.two <- filter(PBNdat,Date_c == '2016-09-08' | Date_c == '2016-09-09')
anova(lm(stdIgf~Date, data = PBN.two))

ggplot(PBN, aes(x = text_date, weight = mean.igf, ymin=mean.igf - sem.igf, ymax=mean.igf + sem.igf, fill = Protection))+
  geom_bar(stat = 'identity', color = 'black', aes(y=mean.igf))+
  geom_errorbar(width = 0.25)+
  ylab('Plasma Igf1 (ng / ml)')+
  xlab('')+
  coord_cartesian(ylim = c(20,35))+
  facet_grid(.~group, scales = 'free_x')+
  scale_fill_manual(values = c("#D10000","#004C99"))+
  geom_signif(data=PBN,aes(y_position=c(31.5,31.5,28,28),xmin=c(0.8,0.8,0.8,0.8), xmax=c(2.2,2.2,2.2,2.2), annotations=c('NS','NS','NS','NS')),tip_length = 0,textsize = 6, manual=T) +
  stat_count(aes(label = n), geom = "text",size = 6, color='white', position=position_dodge(width = 0.6), vjust=2.4)+
  theme_classic(base_size = 18)+
  theme(plot.title = element_text(hjust = 0.5),axis.text.x = element_text(angle = 45, hjust = 1), plot.margin = unit(c(1,1,1.5,1.2),"cm"), strip.text.x = element_blank())
ggsave(filename = 'PBN-igf1.tiff', width = 7, height = 5)
```

*Using square root transformed raw data

First 2 days (08/08-09)
```{r}
aov_pbn1 = lm(igf~Length, data = PBN.one)
anova(aov_pbn1)
```

Last 2 days (09/08-09)
```{r}
aov_pbn2 = lm(igf~Length,data=PBN.two)
anova(aov_pbn2)
```

Comparing between two regressions
```{r}
anova(lm(igf~Length+group, data = PBNdat))
ggplot(PBNdat, aes(x = Length, y=igf, color = group))+
  geom_point()+
  geom_smooth(method = 'lm')+
  scale_color_manual(values = c("orange","brown"), name = '', labels = c('August', 'September'))+
  theme_classic()
```

The groups of dates are significantly different but the issue is that the lengths at these dates are also significantly different

```{r}
summary(lm(Length~group, data = PBNdat))
ggplot(PBNdat, aes(x = group, y=Length, color = group))+
  geom_boxplot()+
  geom_smooth(method = 'lm')+
  scale_color_manual(values = c("orange","brown"))+
  scale_x_discrete(labels = c('August','September'))+
  xlab('')+
  theme_classic()+
  theme(legend.position = 'none')
```

Running regression on *non-normalized* data and saving residuals

August
```{r}
pbn1_res <- lm(stanIGF~Length, data = PBN.one)
summary(pbn1_res)
PBN.one <- PBN.one %>% modelr::add_residuals(pbn1_res)
pls::predplot(pbn1_res)
```

September
```{r}
pbn2_res <- lm(stanIGF~Length, data = PBN.two)
summary(pbn2_res)
PBN.two <- PBN.two %>% modelr::add_residuals(pbn2_res)
pls::predplot(pbn2_res)
```


###Length
```{r}
anova(lm(Length~Date*Protection, data = PBNdat))
```

First 2 days
```{r}
anova(lm(Length~Date, data = PBN.one))
```

Last 2 days
```{r}
anova(lm(Length~Date, data = PBN.two))

ggplot(PBN, aes(x = text_date, weight = mean.len, ymin=mean.len - sem.len, ymax=mean.len + sem.len, fill = Protection))+
  geom_bar(stat = 'identity', color = 'black', aes(y=mean.len))+
  geom_errorbar(width = 0.25)+
  ylab('Length (cm)')+
  xlab('')+
  coord_cartesian(ylim=c(15,25))+
  scale_y_continuous(breaks = c(15,20,25))+
  facet_grid(.~group, scales = 'free_x')+
  scale_fill_manual(values = c("#D10000","#004C99"))+
  geom_signif(data=PBN,aes(y_position=c(23,23,21,21),xmin=c(0.8,0.8,0.8,0.8), xmax=c(2.2,2.2,2.2,2.2), annotations=c('**','**','NS','NS')),tip_length = 0,textsize = 6, manual=T) +
  theme_classic(base_size = 18)+
  stat_count(aes(label = n), geom = "text",size = 6, color='white', position=position_dodge(width = 0.6), vjust=2.4)+
  theme(plot.title = element_text(hjust = 0.5),axis.text.x = element_text(angle = 45, hjust = 1), plot.margin = unit(c(1,1,1.5,1.2),"cm"), strip.text.x = element_blank())
ggsave(filename = 'PBN-length.tiff', width = 7, height = 5)
```

###LSmeans

This is using raw Igf data not residuals.

```{r}
PBN.lm = lm(igf~Date_c + factor(Length), data = PBNdat)
library(emmeans)
PBN.lsm = emmeans(PBN.lm, 'Date_c')
library(multcompView)
cld(PBN.lsm,Letters=letters)#adds letters and makes it readable
```

First 2 days
```{r}
anova(lm(igf~Date_c + factor(Length), data = PBN.one))
```

Last 2 days
```{r}
anova(lm(igf~Date_c + factor(Length), data = PBN.two))

#For graphing
PBN.lm.graph = lm(stanIGF~Date_c+factor(Length), data = PBNdat)
PBN.graph = cld(emmeans(PBN.lm.graph,'Date_c'))
PBN.graph$Protection = c('MPA','REF','REF','MPA')
PBN.graph$group=c(2,2,1,1)
PBN.graph$sig = c('b','ab','ab','a')

ggplot(PBN.graph, aes(x = Date_c, weight = emmean, ymin=emmean - SE, ymax=emmean + SE, fill = Protection))+
  geom_bar(stat = 'identity', color = 'black', aes(y=emmean))+
  geom_errorbar(width = 0.25)+
  ylab('LSmeans Igf1 (ng / ml)')+
  xlab('')+
  coord_cartesian(ylim=c(15,30))+
  facet_grid(.~group, scales = 'free_x')+
  scale_fill_manual(values = c("#D10000","#004C99"))+
  geom_signif(data=PBN.graph,aes(y_position=c(24.5,24.5,21,21),xmin=c(0.8,0.8,0.8,0.8), xmax=c(2.2,2.2,2.2,2.2), annotations=c('NS','NS','NS','NS')),tip_length = 0,textsize = 6, manual=T) +
  theme_classic(base_size = 18)+
  theme(plot.title = element_text(hjust = 0.5),axis.text.x = element_text(angle = 45, hjust = 1), plot.margin = unit(c(2,1,1,1),"cm"), strip.text.x = element_blank())
ggsave(filename = 'PBN-lsmeans.tiff', width = 7, height = 5)
```

#Correlations

```{r Correlations}

#Correlation graph
ggplot(matched, aes(y = stdIgf, x = Length)) + 
  geom_point(aes(fill = Protection), shape = 21, color =('black'), size = 3) +
  scale_fill_manual(name = 'Protection', values = c("#D10000","#004C99"), labels = c('MPA','REF'))+
  stat_smooth(method = 'lm',data = matched ,color = 'black',size = 2, fullrange = T, se = F)+
  scale_x_continuous(expand=c(0,0)) +
  scale_y_continuous(expand=c(0,0)) +
  coord_cartesian(xlim=c(10,40), ylim=c(0,60)) +
  ylab(expression('Plasma Igf1'['STD']))+
  xlab('Length (cm)')+
  theme_classic(base_size = 20)+
  theme(legend.background = element_rect(linetype = 'solid', size = 0.5, colour = 'black'),legend.position = c(0.8,0.9),plot.margin=unit(c(1,1,1.5,1.2),"cm"))
ggsave(filename = 'cor.stdIgf.jpeg', height = 5, width = 7)

cor.matched <- list()
for (q in unique(matched$cDate)){
  igf1 = dplyr::filter(matched, cDate == q)$igf
  length = dplyr::filter(matched, cDate == q)$Length
  correlation = cor.test(igf1,length,method='pearson')
  correlation$data.name = q
  cor.matched = cbind(cor.matched,correlation)
}

knitr::kable(cor.matched[c(1:4,8,9),], caption = 'Correlations of igf1 and length by date')
```

#PCA

```{r, Editting data, include =F}
# Get data from 2016/08/08 (buoy) 
DCbuoy <- read_table("DCbuoy.txt")
names(DCbuoy)[1] <- 'YY' #Get rid of hashtag
#clean data to make datetime column for merging
DCbuoy_clean = DCbuoy %>%
  filter(!is.na(YY)) %>%
  unite(date, YY,MM,DD, sep = '/')%>%
  unite(time, hh,mm, sep = ':')%>%
  unite(datetime, date, time, sep = ' ' )%>%
  mutate(datetime = lubridate::ymd_hm(datetime, tz = 'UTC')) %>% 
  mutate(datetime = lubridate::with_tz(datetime, tzone = "US/Pacific"),month = as.character(lubridate::month(datetime))) %>%
  dplyr::filter(WTMP<75) 

#Select specific dates in buoy data of my sampling dates
buoy_summerT = DCbuoy_clean %>%
  dplyr::select(datetime, WTMP)
buoy_summerT = buoy_summerT[as.Date(buoy_summerT$datetime) %in% unique(matched$Date),]

#Make datetime in my dataframe
temp.test = unite(matched, cDateTime, cDate, St_time, sep = ' ') %>%
  mutate(cDateTime =lubridate::ymd_hms(cDateTime, tz = "US/Pacific" ))
#Find earliest start time and latest end time for each day
collection_times = temp.test %>% group_by(Date) %>%
  dplyr::summarise(Start = min(cDateTime), End = max(cDateTime), n =length(cDateTime))
collection_times$Start = lubridate::floor_date(collection_times$Start, unit = 'hour')
collection_times$End = lubridate::ceiling_date(collection_times$End, unit = 'hour')
#Extract buoy data from the times in collection_times
buoy <- data.frame()
for (n in 1:nrow(collection_times)){
  time = buoy_summerT[buoy_summerT$datetime>collection_times$Start[n] & buoy_summerT$datetime<collection_times$End[n],]
  buoy = rbind(buoy,time)
}
```
```{r, Testing temps}
#See if sources of temperature are different
temps_plot = ggplot(buoy, aes(x = datetime, y = WTMP))+geom_point()+geom_point(data = temp.test, aes(x = cDateTime, y = `Surface T (instrument, C)`), color = 'red')
#Select temps from date with most data 08-09
temp.aug8 = temp.test %>% 
  filter(Date == '2016-08-09')  
buoyTemp_20160809 = buoy[as.Date(buoy$datetime)==as.Date('2016-08-09'),]
#run t-test
testtemps = t.test(temp.aug8$`Surface T (instrument, C)`,buoyTemp_20160809$WTMP)
plot08092016 = ggplot(buoyTemp_20160809, aes(x = datetime, y = WTMP))+geom_point() + geom_point(data = temp.aug8, aes(x = cDateTime, y = `Surface T (instrument, C)`  ), color = 'red')
#average temps and test again
temp.test <- mutate(temp.test, vtemp = ((temp.test$`SWT (vessel, F)` - 32)*(5/9)))#convert to celsius
temp.test$temp <- apply(temp.test[,c(19,35)],1,mean, na.rm=TRUE)#Average tempertures
#See if sources of temperature are different
temps_plot2 = ggplot(buoy, aes(x = datetime, y = WTMP))+geom_point()+geom_point(data = temp.test, aes(x = cDateTime, y = temp), color = 'red')
#Temperatures significantly different so can only use buoy data in PCA
```

```{r}
#Get means per day per hour for buoy data
buoy.sum <- buoy %>% group_by(as.Date(datetime),hour(datetime)) %>% dplyr::summarise(mtemp = mean(WTMP), sdtemp = sd(WTMP), n=length(datetime)) %>% mutate(cDateTime = paste(`as.Date(datetime)`,`hour(datetime)`)) 
buoy.sum$cDateTime = lubridate::ymd_h(buoy.sum$cDateTime)
#make matching column in my data in order to join
temp.test = mutate(temp.test, datehour = paste(Date,hour(cDateTime))) 
temp.test$datehour = lubridate::ymd_h(temp.test$datehour)
#join data sets and make new column 
matched.new = dplyr::left_join(temp.test,buoy.sum, by = c('datehour'='cDateTime')) %>% dplyr::arrange(datehour)#ensure it is in ascending order for fill() to work
matched.new = tidyr::fill(matched.new,mtemp)#fills NA values with value above
#Missing values are due to the buoy missing data 
#Double check data to ensure it duplicates temperatures that are close in time
```

Blue is positive correlation and red is negative correlation

```{r}
#average depths
matched.new$depth <- apply(matched.new[,c(23,24)],1,mean,na.rm=T)
#export for mapping
RFBLU2016 <- matched.new[,c(10,3,4,8,34,15:18,22,25,26,28:31,40,43)]
write_csv(RFBLU2016, 'RFBLU2016.csv')
RFBLU_PBN <- filter(RFBLU2016, Location == 'PBN')
write_csv(RFBLU_PBN, 'RFBLU_PBN.csv')
RFBLU_PBL <- filter(RFBLU2016, Location == 'PBL')
write_csv(RFBLU_PBL, 'RFBLU_PBL.csv')

PCA <- matched.new[,c(22,25,26,28:31,40,43)]#only select columns used in PCA
colnames(PCA)=c('Rel',"cloud",'windsp','swell','waveht','wavedir','secchi','temp','depth')
library(corrplot)
corrplot(cor(PCA, use = 'complete.obs'),method = "circle", order = 'FPC')
```


```{r}
#make dataset of just PCA terms
eco = matched.new[,c(22,25,26,28:31,43,40,34)]
names(eco) = c('Rel',"cloud",'windsp','swell','waveht','wavedeg','secchi','depth','temp','stdigf')
anova(lm(stdigf~Rel+cloud+windsp+swell+waveht+wavedeg+secchi+depth+temp, data = eco))

#Find which variables have most NAs
pca.sum <- eco %>%
  dplyr::summarise_all(funs(sum(is.na(.))))

mtemps = matched.new %>% group_by(Location,Date,sicell,Drift) %>% dplyr::summarise(mtemp = mean(mtemp, na.rm=T),n = length(mtemp))
```

```{r}
Loc <- matched.new[complete.cases(PCA),]$Location#Remove NAs
cDate <- matched.new[complete.cases(PCA),]$Date
Protect <- matched.new[complete.cases(PCA),]$Protection
#Run PCA
blu.pca <- prcomp(na.omit(PCA), center = T, scale. = T)
#summary(blu.pca)
```


```{r, include = F}
library(ggbiplot)
ggbiplot(blu.pca, obs.scale = 1, var.scale = 1, 
              groups = Loc, ellipse = TRUE, 
              circle = TRUE)+
  scale_color_discrete(name = '')+
  theme_classic()
ggbiplot(blu.pca, obs.scale = 1, var.scale = 1, 
              groups = as.character(cDate), ellipse = TRUE, 
              circle = TRUE)+
  scale_color_discrete(name = '')+
  theme_classic()
ggbiplot(blu.pca, obs.scale = 1, var.scale = 1, 
              groups = Protect, ellipse = TRUE, 
              circle = TRUE)+
  scale_color_discrete(name = '')+
  theme_classic()
```

PCA not including relief and cloud cover as they are catagorical

```{r}
mPCA <- PCA[,c(3:9)]
blu2.pca <- prcomp(na.omit(mPCA), center = T, scale. = T)
summary(blu2.pca)
Kaiser = (blu2.pca$sdev)^2#Kaiser’s criterion: that we should only retain principal components for which the variance is above 1
kaiser = as.data.frame(Kaiser) 
knitr::kable(kaiser, caption = 'Kaiser values for PC1-7')
```

We need 5 PCs to explain 90% of the variance but using Kaiser's criterion we should only retain the first 3 PCs

```{r, PCA graphs}
ggbiplot(blu2.pca, obs.scale = 1, var.scale = 1, 
              groups = as.character(cDate), ellipse = TRUE, 
              circle = TRUE)+
  scale_color_discrete(name = '')+
  theme_classic()
ggbiplot(blu2.pca, obs.scale = 1, var.scale = 1, 
              groups = Protect, ellipse = TRUE, 
              circle = TRUE)+
  scale_color_discrete(name = '')+
  theme_classic()
ggbiplot(blu2.pca, obs.scale = 1, var.scale = 1, 
              groups = Loc, ellipse = TRUE, 
              circle = TRUE)+
  scale_color_discrete(name = '')+
  theme_classic()
```


Adding PCAs to model

Including cloud cover and relief:

```{r}
dat_lenPCA = lm(stdIgf~blu.pca$x, data = matched.new[complete.cases(PCA),])
summary(dat_lenPCA)
```

Excluding cloud cover and relief:

```{r}
pca.lm2 = lm(stdIgf~blu2.pca$x, data = matched.new[complete.cases(mPCA),])
summary(pca.lm2)
```

We should definitely include PC1-3 as they are always significant predictors. What do you think about PC5?

```{r}
# Plot PC's 1 and 5:
ggbiplot(blu2.pca, choices = c(1,5), obs.scale = 1, var.scale = 1, groups =Protect, ellipse = TRUE, circle = TRUE)+scale_color_discrete(name = '')+theme_classic()
```

Only including PC1-3+5 in the final model

```{r}
res.mod = lm(stdIgf~blu2.pca$x[,1]+blu2.pca$x[,2]+blu2.pca$x[,3]+blu2.pca$x[,5], data = matched.new[complete.cases(mPCA),])
summary(res.mod)
anova(pca.lm2,res.mod)
eco <- eco[,3:10]
neco <- cbind(eco[complete.cases(mPCA),],blu2.pca$x)%>%gather('PCnum','PCval',9:15)
ggplot(data = neco, aes(x=stdigf, y=PCval))+
  geom_point()+
  geom_smooth(method = 'lm')+
  facet_wrap(~PCnum, scales = 'free_y')+
  theme_classic()
#Make table of loadings
knitr::kable(blu2.pca$rotation, caption = 'PCA loadings')
```


```{r, PCR}
#Run principle componenet regression against igf
#remove cloud cover and relief from data
library(pls)
pcr <- pcr(stdigf~.,data = eco[complete.cases(eco),], scale = TRUE, validation = 'CV')
validationplot(pcr,  val.type = "R2")
pls::predplot(pcr, ncomp = 5, line=T)
```


```{r, PCR graph, include=F}
pcr.df <- data.frame(pcr$scores[,1],pcr$scores[,2]) 
names(pcr.df) = c('Comp1','Comp2')
ggplot(data=pcr.df,aes(x=Comp1,y=Comp2)) + 
  geom_point(aes(fill=Loc),shape=21,size=3)+
  theme_classic()
ggplot(data=pcr.df,aes(x=Comp1,y=Comp2)) + 
  geom_point(aes(fill=Protect),shape=21,size=3)+
  theme_classic()
ggplot(data=pcr.df,aes(x=Comp1,y=Comp2)) + 
  geom_point(aes(fill=cDate),shape=21,size=3)+
  scale_fill_gradient(low = 'red', high = 'green')+
  theme_classic()

```

```{r}
#Determining how much time has passed
#timespan = min(temp.test$cDateTime) %--% max(temp.test$cDateTime)
#buoy_test <- buoy_summerT[buoy_summerT$datetime %within% timespan,]
```

```{r, Heat map}
##Heat Map
# myloc = c(lon = -121.0227,lat = 35.5086)
# slo <- get_map(location = myloc,source = 'google', maptype = 'hybrid')
# ggmap(slo)+
#   geom_density2d(data = matched,aes(x = ST_LonDD, y = ST_LatDD))+
#   stat_density2d(data = matched,aes(x = ST_LonDD, y = ST_LatDD, fill=..level.., alpha=igf), size = 0.01,
#     bins = 10, geom = "polygon") +
#   scale_fill_gradient(low = "green", high = "red") +
#   scale_alpha(range = c(0,5), guide = FALSE)
```

```{r, PBN Heat map}
##Point Buchon
# #make sure it is plotting igf and not counts
# pbnloc = c(lon = -120.9,lat = 35.24)
# pbn <- get_map(location = pbnloc,source = 'google', maptype = 'hybrid', zoom = 12)
# #adding mpa polygon
# library(raster)
# shpData <- raster::shapefile("C:/Users/manic/OneDrive/Documents/RFOYT/Shapefiles/MPA_CA_Existing_160301.shp")
# shpData <- spTransform(shpData,CRS("+proj=longlat +datum=WGS84"))#trnasform shp from Nad to wgs
# 
# ggmap(pbn)+
#   geom_polygon(data = shpData,aes(x = long, y = lat), fill = 'red', alpha = 0.5)+
#   #geom_density2d(data = PBNdat,aes(x = ST_LonDD, y = ST_LatDD))+
#   #stat_density2d(data = PBNdat,aes(x = ST_LonDD, y = ST_LatDD, fill=..level.., alpha = ..level..), size = 0.1, bins = 16, geom = "polygon") +
#   geom_tile(aes(x = End_LonDD, y = End_LatDD, color = stanIGF),data = PBNdat,size = 3)+
# #   scale_fill_gradientn(colours = heat.colors(6))
# #   scale_alpha(range = c(0, 1), guide = FALSE)+
# #   scale_fill_gradient(low = "white", high = "dark green")
# # geom_point(data = PBNdat, aes(x = ST_LonDD, y = ST_LatDD), size = PBNdat$stanIGF, bins = 20)
# # add habitat information on top
```

